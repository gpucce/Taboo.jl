[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taboo",
    "section": "",
    "text": "Build cards using Wikipedia and draw them.\nCheck the docs and see how it works.\n\nfrom PIL import Image\nfrom Taboo import draw_card, load_taboo_words, add_taboo_words\n\n\nprint(\"organic farming\")\nImage.open(\"./test_data/raw_images/organic_farming_raw.png\")\n\norganic farming\n\n\n\n\n\n\nim = draw_card(\"./test_data/raw_images/organic_farming_raw.png\")\nim\n\n\n\n\n\ntaboo_words = load_taboo_words(\"./test_data/taboo_cards/Organic farming.json\")[\"cards\"]\ntaboo_words[:2]\n\n[{'Manure': ['Organic matter',\n   'Organic fertilizer',\n   'Agriculture',\n   'Feces',\n   'Compost',\n   'Green manure',\n   'Soil fertility',\n   'Nutrient',\n   'Nitrogen',\n   'Bacteria']},\n {'Green manure': ['Agriculture',\n   'Biomass (ecology)',\n   'Manure',\n   'Soil organic matter',\n   'Legume',\n   'Organic farming',\n   'Intensive farming']}]\n\n\n\nadd_taboo_words(im, taboo_words[0])\n\n\n\n\n\nadd_taboo_words(im, taboo_words[2])"
  },
  {
    "objectID": "scraper.html",
    "href": "scraper.html",
    "title": "Scraper",
    "section": "",
    "text": "WikiPage\n\n WikiPage (url, wiki_url)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nclass Crawler():\n    def __init__(self, output_path, lang=\"en\", first_n=10):\n        self.lang = lang\n        self.search_url = f\"https://{lang}.wikipedia.org/w/index.php?search=\"\n        self.wiki_url = f\"https://{lang}.wikipedia.org/\"\n        self.pages = {}\n        self.first_n = first_n\n        self.output_path = Path(output_path)\n        if not self.output_path.exists():\n            self.output_path.mkdir(parents=True, exist_ok=True)\n    \n    def create_page_from_query(self, query):\n        url = self.search_url + query.replace(\" \", \"+\")\n        return self.create_page(url)\n\n    def create_page(self, url):\n        return WikiPage(url, wiki_url=self.wiki_url)\n\n    def create_card(self, url, final=False):\n        starting_page = self.create_page(url)\n        out = {\"starting_page\": starting_page, \"children_pages\":{}}\n        count = 0\n        for url in out[\"starting_page\"].get_beginning_links():\n            try:\n                if final:\n                    out[\"children_pages\"][url] = self.create_page(url)\n                else:\n                    out[\"children_pages\"][url] = None\n            except:\n                continue\n            count += 1\n            if len(out[\"children_pages\"]) >= self.first_n:\n                break\n        return out\n    \n    def create_cards(self, query):\n        url = self.search_url + query.replace(\" \", \"+\")\n        cards = self.create_card(url, final=False)\n        out = {\"query\": str(cards[\"starting_page\"]), \"cards\":[]}\n        for url in cards[\"children_pages\"].keys():\n            try:\n                new_card = self.create_card(url, final=True)\n            except:\n                continue\n            out[\"cards\"].append({str(new_card[\"starting_page\"]): [str(i) for i in new_card[\"children_pages\"].values()]})\n        return out\n\n    def save_cards(self, cards):\n        with open(self.output_path / f\"{cards['query']}.json\", \"w\") as f:\n            json.dump(cards, f)\n\n\n\n\nCrawler\n\n Crawler (output_path, lang='en', first_n=10)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nTests\n\nquery = \"ecology\"\ncrawler = Crawler(\"./test_data/temp_files\", \"en\")\npage = crawler.create_page_from_query(\"football\")\ncards = crawler.create_cards(query)\ncrawler.save_cards(cards)\n\ntest_eq(crawler.search_url + \"football\", \"https://en.wikipedia.org/w/index.php?search=football\")\ntest_eq(crawler.wiki_url + \"wiki/football\", \"https://en.wikipedia.org/wiki/football\")\ntest_eq(page.url, \"https://en.wikipedia.org/w/index.php?search=football\")\nassert os.path.exists(f\"./test_data/temp_files/{query.replace(' ', '_').capitalize()}.json\")\n# rmtree(\"./test_data/temp_files\")"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Drawer",
    "section": "",
    "text": "draw_card\n\n draw_card (img_path, height=420, width=420)\n\nLoads an image, rescales it and adds a border.\n\ndef load_taboo_words(file_path):\n    with open(file_path, 'r') as f:\n        out = json.load(f)\n    return out\n\n\n\n\nload_taboo_words\n\n load_taboo_words (file_path)\n\n\ndef add_taboo_words(img, words):\n    \"\"\"\n    Adds the taboo words to an image to make it a Taboo card.\n    \"\"\"\n    top_w, forb_ws = list(words.items())[0]\n    top_w = _capitalize_all(top_w)\n    forb_ws = [_capitalize_all(w) for w in forb_ws]\n    new_im = Image.new(\"RGBA\", img.size)\n    draw = ImageDraw.Draw(new_im)\n    draw.rounded_rectangle([(50, 50), (img.width-50, 100)], fill = (255, 255, 255, 50), radius=20)\n    start_font_size = 10\n    fontsize = start_font_size\n    font = _resize_font(top_w, img.width-100, 50)\n    draw.text((img.width // 2, 75), top_w, fill=(0, 0, 0, 255), anchor=\"mm\", font=font)\n\n    for idx, forb_w in enumerate(forb_ws[:5]):\n        down_shift = (50 / 2.5) + (idx + 1) * (50)\n        draw.rounded_rectangle([(70, 50 + down_shift), (img.width-70, 90 + down_shift)], fill = (255, 255, 255, 50), radius=20)\n        font = _resize_font(forb_w, img.width-140, 40)\n        draw.text((img.width // 2, 70 + down_shift), forb_w, fill=(0, 0, 0, 255), anchor=\"mm\", font=font)\n\n    return Image.alpha_composite(img, new_im,)\n\n\n\n\nadd_taboo_words\n\n add_taboo_words (img, words)\n\nAdds the taboo words to an image to make it a Taboo card."
  }
]